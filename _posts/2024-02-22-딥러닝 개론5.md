---
title: 05. 딥러닝 개론 - 신경망(neural networks)
author: guno
date: 2024-02-22 00:00:00 +0800
categories: [Computer Vision, 딥러닝 개론]
tags: [딥러닝 기초]
img_path:
---

## 선형회귀(linear regression)

- 가중합을 통해서 어떤 새로운 특성을 생성하는 구조
- 새로운 특성을 가지고 다시 어떤 새로운 가중합과 바이어스 값을 통해서 다시 타겟을 예측하는 구조가 신경망 모델이다.

### 신경망 모델
- 구조는 커스텀하기 나름
- 가장 중요한 부분은 선형회귀 구조를 이용해서 새로운 특성을 만들어내는 것이 핵심. 몇개의 특성을 만들어내는지는 설정하기 나름이고 얼마든지 더 많은 노드를 설정해서 새로운 특성들을 많이 생성할 수가 있고, 레이어를 깊게 쌓아서 여러개의 레이어를 만들어 낼수가 있고, 레이어가 깊어지는 것을 딥러닝이라고 한다.
- 노드의 개수가 많아지면 더 많은 가중치 조합을 이용해서 새로운 특성을 생성하는 것이기 때문에 좀 더 디테일하고 자세한 정보 혹은 데이터의 분포에 대한 특성을 추출한다고 볼 수 있다. 
- input - layer1  - layer2 -  output
- 신경망을 깊이 쌓으면 쌓을 수록 많은 정보들을 담고 있다.
- 하지만 너무 지나치게 깊고 복잡한 모델은 과적합 가능성이 있다. 일반화 성능이 떨어질 수 있다.
- 그러므로 지나치게 복잡한 모델보다는 단순하고 가볍고 빠른 모델을 가지고 실험하는 것이 좀 더 선호되는 경향이 있다.

### 비선형변환
- 가장 유명한 비선형 변환인 시그모이드 함수
- 실수 전체 범위를 input 으로 받아서 0~1 사이의 값으로 반환, threshold, 스위치의 역할도 할 수 있다
- 시그모이드 변환을 통해서 어떤 값을 출력한 다음에 그 값을 다음층으로 전파함. 그럼으로써 신경망을 깊은 구조로 쌓더라도 그 의미가 하나의 레이어로 대체될 수 있는 신경의 조합이라기 보다는 비선형이 껴있기 때문에 깊은 층들을 모두 활용할 수 있다.
- 그런 의미해서 비선형변환은 꼭 필요하다고 할 수 있다.

### Activation
- 이전 층의 비선형 변환을 통한 계산 값을 activation 이라고 한다.

### 활성화 함수(activation function)
- Sigmoid, ReLU
- ReLU(Rectified Linear Unit): X가 0보다 작은 부분에 한해서는 F(x)=0 이다. 그리고 X 가 0보다 크거나 같은 부분에 대해서는 F(x)=x 이다.
- 계산이 시그모이드 함수보다 훨씬 더 단순하다는 장점이 있다.
- Sigmoid 의 경우 gradient vanishing 이라는 기울기 소실의 문제가 있다. X 의 절대값이 클수록 기울기가 0에 가깝게 되고 여기에 곱해지는activation Function 에 기울기 값이 0 이되면 전체 값이 0이 되면 더이상 이 이후에는 학습이 일어날 수가 없다. 가중치가 업데이트 될 수 없다. 그러한 Gradient Vanishing 은 학습의 저하 문제가 발생한다. 
- 이러한 면에서 ReLU 는 훨씬 더 간편하면서 컴퓨팅도 빠르고 Gradient Vanishing 도 유발하지 않는다. 기울기값이 1이 곱해지는 구조이기 때문에 Gradient Vanishing 문제를 해결했다고 할 수 있다.




[nodejs]: https://nodejs.org/
[starter]: https://github.com/cotes2020/chirpy-starter
[pages-workflow-src]: https://docs.github.com/en/pages/getting-started-with-github-pages/configuring-a-publishing-source-for-your-github-pages-site#publishing-with-a-custom-github-actions-workflow
[latest-tag]: https://github.com/cotes2020/jekyll-theme-chirpy/tags
